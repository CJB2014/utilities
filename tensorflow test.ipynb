{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_val = mnist.validation.images\n",
    "y_val = mnist.validation.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],-1).T\n",
    "y_train = y_train.reshape(y_train.shape[0],-1).T\n",
    "X_val = X_val.reshape(X_val.shape[0],-1).T\n",
    "y_val = y_val.reshape(y_val.shape[0],-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 55000)\n",
      "(10, 55000)\n",
      "(784, 5000)\n",
      "(10000, 784)\n",
      "Number a training example: 55000\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print('Number a training example: ' + str(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_creation(n_x,n_y):\n",
    "    X = tf.placeholder(tf.float32, shape = [n_x,None], name = 'X')\n",
    "    Y = tf.placeholder(tf.float32, shape = [n_y,None], name = 'Y')\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "def initialize_parameters():\n",
    "    W1 = tf.get_variable('W1',[128,784], initializer = tf.contrib.layers.xavier_initializer(seed = 3))\n",
    "    b1 = tf.get_variable('b1',[128,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable('W2',[40,128], initializer = tf.contrib.layers.xavier_initializer(seed = 3))\n",
    "    b2 = tf.get_variable('b2', [40,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable('W3',[10,40], initializer = tf.contrib.layers.xavier_initializer(seed = 3))\n",
    "    b3 = tf.get_variable('b3',[10,1], initializer = tf.zeros_initializer())\n",
    "    parameters = {\n",
    "        'W1':W1,\n",
    "        'b1':b1,\n",
    "        'W2':W2,\n",
    "        'b2':b2,\n",
    "        'W3':W3,\n",
    "        'b3':b3\n",
    "    }\n",
    "    return parameters\n",
    "\n",
    "def forward_prop(X,parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3)\n",
    "    return Z3 \n",
    "\n",
    "def compute_cost(Z3,Y):\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    return cost \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(128, 784) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(128, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(40, 128) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(40, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 32, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "    \n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    \n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)     \n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, mini_batch_size * num_complete_minibatches : m]\n",
    "        mini_batch_Y = shuffled_Y[:, mini_batch_size * num_complete_minibatches : m]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_val, y_val, learning_rate = 0.001, num_epochs = 1500, minibatch_size = 32 , print_cost = True):\n",
    "    ops.reset_default_graph()\n",
    "    tf.set_random_seed(3)\n",
    "    seed = 3\n",
    "    (n_x,m) = X_train.shape\n",
    "    n_y = y_train.shape[0]\n",
    "    costs = []\n",
    "    \n",
    "    X,Y = placeholder_creation(n_x,n_y)\n",
    "    \n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    Z3 = forward_prop(X,parameters)\n",
    "    \n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m/minibatch_size)\n",
    "            seed = seed + 1 \n",
    "            minibatches = random_mini_batches(X_train,y_train,minibatch_size, seed)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X,minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = sess.run([optimizer,cost], feed_dict = {X: minibatch_X, Y: minibatch_Y})\n",
    "                epoch_cost += minibatch_cost/num_minibatches\n",
    "            if print_cost == True and epoch % 100 == 0 :\n",
    "                print('Cost after epoch %i: %f' %(epoch, epoch_cost))\n",
    "            if print_cost ==True and epoch%5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iteration (per tens)')\n",
    "        plt.title('Learning rate = ' + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        parameters = sess.run(parameters)\n",
    "        print('Paramateres have been trained')\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(Z3),tf.argmax(Y))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "        \n",
    "        print('train', accuracy.eval({X:X_train, Y:y_train}))\n",
    "        print('eval',accuravy.eval({X:X_val, Y:y_val}))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.877333\n",
      "Cost after epoch 100: 0.113026\n",
      "Cost after epoch 200: 0.058836\n",
      "Cost after epoch 300: 0.034412\n",
      "Cost after epoch 400: 0.021098\n",
      "Cost after epoch 500: 0.013456\n",
      "Cost after epoch 600: 0.008974\n",
      "Cost after epoch 700: 0.006295\n",
      "Cost after epoch 800: 0.004638\n",
      "Cost after epoch 900: 0.003579\n",
      "Cost after epoch 1000: 0.002869\n",
      "Cost after epoch 1100: 0.002365\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
